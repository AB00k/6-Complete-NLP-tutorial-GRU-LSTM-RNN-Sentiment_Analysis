{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "You can use your own way of preprocessing to enhance results. Best results will lead to bonus points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    lower = text.lower()\n",
    "    # Removing Punctuation marks\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    rem_punc = tokenizer.tokenize(lower)\n",
    "    # Removing Stop Words\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    rem_stop_words = [word for word in rem_punc if not word in stopwords]\n",
    "    # Removing Non-English words \n",
    "    english_words = nltk.corpus.words.words()\n",
    "    english_words = [word for word in rem_stop_words if word in english_words]   \n",
    "    # Insert Start End tokens\n",
    "    english_words.insert(0,'<start>')\n",
    "    english_words.append('<end>')\n",
    "    sentence = ' '.join(english_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    # Tokenization\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    # Converting to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    # Padding Zeros \n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
    "    \n",
    "    return padded_sequences, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(map(preprocess,reviews.review[:10]))\n",
    "encodings, tokenizer = encode_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encodings\n",
    "y = np.where(np.array(reviews.sentiment)=='positive',1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence models\n",
    "Following is an example code for simple LSTMs containing one layer. Your implementation should be generic in which user can be able to create multiple layers if required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, hidden_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        # use pytorch tensors\n",
    "        # require_grad parameter should be True \n",
    "        # Instead of following you can also use nn.Linear()\n",
    "        self.input_dim  = ??\n",
    "        self.hidden_dim = ??\n",
    "        self.W_x = ??\n",
    "        self.W_h = ??\n",
    "        self.b_x = ??\n",
    "        self.b_h = ??\n",
    "\n",
    "    def lstm_step(self, inp, prev_hidden_cell):     \n",
    "        \n",
    "        h_prev , c_prev = prev_hidden_cell\n",
    "        \n",
    "        # The activation vector\n",
    "        activation = ??\n",
    "        \n",
    "        # The activation is split into four parts\n",
    "        ai, af, ac, ao = ??\n",
    "\n",
    "        updated_h, updated_c = None, None\n",
    "      \n",
    "        # TODO: Implement the gates of lstm and update hidden state and cell state\n",
    "\n",
    "        in_gate     = ??\n",
    "        forget_gate = ??\n",
    "        cell_gate   = ??\n",
    "        out_gate    = ??\n",
    "\n",
    "        updated_c  = ??\n",
    "        updated_h  = ??\n",
    "        \n",
    "        return updated_h, updated_c\n",
    "\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        # Initialize hidden state with zeros (batch_size, hidden_dim)\n",
    "        h = ??\n",
    "        \n",
    "        # Initialize cell state with zeros (batch_size, hidden_dim)\n",
    "        c = ??\n",
    "        \n",
    "        # Loop through the whole sequence and update h_t and c_t at every time step\n",
    "        # Shape of x is (batch_size, dim)\n",
    "        for x in inp:\n",
    "            h, c = self.lstm_step( x, (h, c) )\n",
    "        \n",
    "        return h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(??)\n",
    "loss = ??\n",
    "lr = ??\n",
    "optimizer = ??\n",
    "# in training function you can use same functions for back prop as you used in assignment 1\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "# for testing and validation use with torch.no_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud that shows the positive and negative words and thier frequence \n",
    "# make a dictionary of words with their word_count\n",
    "counts = tokenizer.word_counts\n",
    "# use you trained model to prodict sentiment of these words\n",
    "??\n",
    "# Saperate positive sentiment words and negative sentiment words with their counts \n",
    "??\n",
    "# make word cloud according\n",
    "plt.figure(figsize=(??,??))\n",
    "wordcloud = WordCloud(background_color=??,max_words=??).generate_from_frequencies(??)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud that shows main words used in reviews that are positive or negative  \n",
    "# Randomly select one sentence that has positive sentiment and one with negative sentiment \n",
    "??\n",
    "# make word cloud of those sentences and see what are main words used in positive review and in negative reviews\n",
    "# Similarly you can make word clouds of all positive reviews and negative reviews to see the most repeating words in positive sentiments and negative sentiments\n",
    "plt.figure(figsize=(??,??))\n",
    "plt.title('Prediceted Sentiment on this review: '+??)\n",
    "wordcloud = WordCloud(background_color='White',max_words=100).generate(??)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
